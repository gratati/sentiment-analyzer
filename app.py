import streamlit as st
import joblib
import torch
from transformers import BertTokenizer, BertModel
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import plotly.express as px
from wordcloud import WordCloud
from collections import Counter
import os

# === –ü—É—Ç–∏ ===
MODEL_DIR = "/content/model_files/"

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ===
@st.cache_resource
def load_models():
    vectorizer = joblib.load(os.path.join(MODEL_DIR, "vectorizer_tfidf.pkl"))
    stacked_model = joblib.load(os.path.join(MODEL_DIR, "stacked_model_tfidf.pkl"))
    final_model = joblib.load(os.path.join(MODEL_DIR, "final_sentiment_classifier.pkl"))
    label_encoder = joblib.load(os.path.join(MODEL_DIR, "label_encoder.pkl"))
    return vectorizer, stacked_model, final_model, label_encoder

@st.cache_resource
def load_bert():
    MODEL_NAME = "DeepPavlov/rubert-base-cased"
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    bert_model = BertModel.from_pretrained(MODEL_NAME)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    bert_model.to(device)
    bert_model.eval()
    return tokenizer, bert_model, device

vectorizer, stacked_model, final_model, label_encoder = load_models()
tokenizer, bert_model, device = load_bert()

# === –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ BERT ===
def get_bert_embeddings(texts):
    if isinstance(texts, str):
        texts = [texts]
    tokens = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = bert_model(**tokens)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
    return cls_embeddings.cpu().numpy()

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ===
def predict_sentiment(texts):
    input_is_str = isinstance(texts, str)
    if input_is_str:
        texts = [texts]

    X_tfidf = vectorizer.transform(texts)
    X_stack = stacked_model.predict_proba(X_tfidf)
    X_bert = get_bert_embeddings(texts)
    X_final = np.hstack([X_stack, X_bert])
    preds = final_model.predict(X_final)
    probs = final_model.predict_proba(X_final)

    pred_labels = label_encoder.inverse_transform(preds)
    confidence_values = np.max(probs, axis=1) * 100
    levels = ["–ù–∏–∑–∫–∞—è" if conf < 65 else "–°—Ä–µ–¥–Ω—è—è" if conf < 85 else "–í—ã—Å–æ–∫–∞—è" for conf in confidence_values]

    if input_is_str:
        return pred_labels[0], confidence_values[0], levels[0]
    else:
        return pred_labels, confidence_values, levels

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
@st.cache_data
def load_test_data():
    try:
        df = pd.read_csv("test.csv")
        if "review_text" in df.columns and "date" in df.columns and "group_name" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors='coerce')
            return df
        else:
            st.error("–û–∂–∏–¥–∞–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ 'review_text', 'date', 'group_name' –≤ test.csv.")
            return pd.DataFrame()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ test.csv: {e}")
        return pd.DataFrame()

df_test = load_test_data()

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ===
st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤", layout="wide")
st.title("üß† –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ + –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
st.markdown("**–ú–æ–¥–µ–ª—å:** TF-IDF + Stacking + BERT")

# === –í–≤–æ–¥ –∏ –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ ===
st.markdown("---")
st.header("üìù –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞")

with st.form("sentiment_form"):
    text_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤:", height=150, key="review_input")
    col1, col2 = st.columns([1, 1])
    with col1:
        analyze_button = st.form_submit_button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å")
    with col2:
        clear_button = st.form_submit_button("–û—á–∏—Å—Ç–∏—Ç—å")

if clear_button:
    st.session_state.pop("review_input", None)
    st.session_state.pop("sentiment_result", None)

if analyze_button:
    if text_input.strip() == "":
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç.")
    else:
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º..."):
            sentiment, confidence, level = predict_sentiment(text_input)
            result = {
                "sentiment": sentiment,
                "confidence": confidence,
                "level": level
            }
            st.session_state["sentiment_result"] = result

if "sentiment_result" in st.session_state:
    result = st.session_state["sentiment_result"]
    st.markdown(f"### üîç –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: **{result['sentiment']}**")
    st.markdown(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {result['level']} ({result['confidence']:.2f}%)")
    st.progress(int(result['confidence']))

    if st.button("üóëÔ∏è –°–∫—Ä—ã—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞"):
        st.session_state.pop("sentiment_result", None)

# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ===
st.markdown("---")
st.header("üìÖ –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")

if not df_test.empty:
    min_date = df_test["date"].min().date()
    max_date = df_test["date"].max().date()
    unique_groups = df_test["group_name"].unique().tolist()
else:
    min_date = max_date = None
    unique_groups = []

date_range = st.date_input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç", value=(min_date, max_date) if min_date and max_date else ())

selected_group = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é", ["–í—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"] + unique_groups)

# === –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ===
filtered_df = df_test.copy()

if isinstance(date_range, tuple) and len(date_range) == 2:
    start_date, end_date = date_range
    filtered_df = filtered_df[(filtered_df["date"].dt.date >= start_date) & (filtered_df["date"].dt.date <= end_date)]

if selected_group != "–í—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏":
    filtered_df = filtered_df[filtered_df["group_name"] == selected_group]

# === –ê–Ω–∞–ª–∏–∑ –≤—ã–±–æ—Ä–∫–∏ ===
st.markdown("---")
st.header("üìä –ê–Ω–∞–ª–∏–∑ –≤—ã–±–æ—Ä–∫–∏")

num_samples = st.slider(
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
    min_value=10,
    max_value=len(filtered_df) if not filtered_df.empty else 10,
    value=min(50, len(filtered_df)) if not filtered_df.empty else 10
)

if not filtered_df.empty and st.button("üìÖ –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"):
    samples = filtered_df["review_text"].sample(min(num_samples, len(filtered_df))).tolist()
    labels, confidences, _ = predict_sentiment(samples)
    results = []

    X_tfidf = vectorizer.transform(samples)
    X_stack = stacked_model.predict_proba(X_tfidf)
    X_bert = get_bert_embeddings(samples)
    X_final = np.hstack([X_stack, X_bert])
    probs = final_model.predict_proba(X_final)

    try:
        pos_class_idx = list(label_encoder.classes_).index("positive")
    except ValueError:
        try:
            pos_class_idx = list(label_encoder.classes_).index("Positive")
        except ValueError:
            pos_class_idx = 0

    positive_probs = probs[:, pos_class_idx]

    for i, text in enumerate(samples):
        results.append({
            "–¢–µ–∫—Å—Ç": text[:100] + "...",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞": labels[i],
            "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (%)": round(confidences[i], 2),
            "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å": round(positive_probs[i], 2)
        })

    df_results = pd.DataFrame(results)
    st.session_state["df_results"] = df_results
    st.session_state["show_results"] = True

if "df_results" in st.session_state and st.session_state.get("show_results", False):
    df_results = st.session_state["df_results"]

    st.dataframe(df_results)

    st.subheader("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
    st.markdown(
        "–ì—Ä–∞—Ñ–∏–∫ –Ω–∏–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ –∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É**. "
        "–ï—Å–ª–∏ –ø–∏–∫–∏ –±–ª–∏–∂–µ –∫ 0.5, –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ —Å–æ–º–Ω–µ–≤–∞–µ—Ç—Å—è; –µ—Å–ª–∏ –±–ª–∏–∂–µ –∫ 0 –∏–ª–∏ 1 ‚Äî –æ–Ω–∞ —É–≤–µ—Ä–µ–Ω–∞."
    )
    plt.figure(figsize=(6, 4))
    sns.histplot(df_results["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"], bins=10, kde=True, color="green")
    plt.xlabel("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞")
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
    st.pyplot(plt)

    st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
    tone_counts = df_results["–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞"].value_counts().reset_index()
    tone_counts.columns = ["–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"]

    color_map = {
        "positive": "#2CA02C",
        "negative": "#D62728",
        "neutral": "#FF7F0E",
        "Positive": "#2CA02C",
        "Negative": "#D62728",
        "Neutral": "#FF7F0E"
    }

    fig_tone = px.bar(
        tone_counts,
        x="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
        y="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
        color="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
        title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
        color_discrete_map=color_map
    )
    st.plotly_chart(fig_tone)

    st.subheader("üîç –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
    low_conf_df = df_results[df_results["–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (%)"] < 65]
    if not low_conf_df.empty:
        st.markdown(f"–ù–∞–π–¥–µ–Ω–æ {len(low_conf_df)} –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤:")
        st.dataframe(low_conf_df)
        st.download_button(
            label="üìÖ –°–∫–∞—á–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ",
            data=low_conf_df.to_csv(index=False).encode("utf-8"),
            file_name="low_conf_reviews.csv",
            mime="text/csv"
        )
    else:
        st.success("–í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—à–µ 65% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏!")

    st.download_button(
        label="üìÅ –°–∫–∞—á–∞—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
        data=df_results.to_csv(index=False).encode("utf-8"),
        file_name="predictions.csv",
        mime="text/csv"
    )

    if st.button("üóëÔ∏è –°–∫—Ä—ã—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–±–æ—Ä–∫–∏"):
        st.session_state["show_results"] = False

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA) ===
st.markdown("---")
st.header("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA)")

if not filtered_df.empty:
    if st.button("üñºÔ∏è –ü–æ—Å—Ç—Ä–æ–∏—Ç—å 2D-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é"):
        texts = filtered_df["review_text"].sample(min(300, len(filtered_df))).tolist()
        labels, _, _ = predict_sentiment(texts)

        embeddings = get_bert_embeddings(texts)
        pca = PCA(n_components=2)
        reduced = pca.fit_transform(embeddings)

        df_viz = pd.DataFrame({
            "X": reduced[:, 0],
            "Y": reduced[:, 1],
            "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": labels,
            "–¢–µ–∫—Å—Ç": [t[:80] + "..." for t in texts]
        })

        fig = px.scatter(
            df_viz,
            x="X",
            y="Y",
            color="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
            color_discrete_map=color_map,
            hover_data=["–¢–µ–∫—Å—Ç"],
            title="–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ (PCA 2D)",
            labels={"X": "–ü–µ—Ä–≤–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞", "Y": "–í—Ç–æ—Ä–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"}
        )
        fig.update_traces(marker=dict(size=10, line=dict(width=1, color='DarkSlateGrey')))

        st.session_state["pca_fig"] = fig
        st.session_state["pca_explained"] = sum(pca.explained_variance_ratio_)

    if "pca_fig" in st.session_state:
        st.plotly_chart(st.session_state["pca_fig"], key="pca_plot")
        st.markdown(f"üîç PCA –æ–±—ä—è—Å–Ω—è–µ—Ç: {st.session_state['pca_explained']:.1%} –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
        st.markdown(
            "–î–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç **–æ—Ç–∑—ã–≤—ã –≤ 2D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ**, —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–º —Å –ø–æ–º–æ—â—å—é PCA.\n"
            "–¢–æ—á–∫–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç **–≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç—å** —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –æ—Ç–∑—ã–≤–∞–º–∏."
        )

# === –û–±–ª–∞–∫–∞ —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º ===
st.markdown("---")
st.header("‚òÅÔ∏è –û–±–ª–∞–∫–∞ —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")

if not filtered_df.empty and st.button("üé® –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤"):
    texts = filtered_df["review_text"].sample(min(300, len(filtered_df))).tolist()
    labels, _, _ = predict_sentiment(texts)

    for i, tone in enumerate(set(labels)):
        tone_texts = [texts[j] for j in range(len(texts)) if labels[j] == tone]
        full_text = " ".join(tone_texts).lower()

        if full_text.strip():
            words = [word for word in full_text.split() if len(word) > 3 and word.isalpha()]
            word_freq = Counter(words).most_common(5)

            wc = WordCloud(width=800, height=300, background_color="white", colormap="viridis").generate(full_text)

            st.markdown(f"#### üåü {tone}")
            st.markdown("**–¢–æ–ø-5 —Å–ª–æ–≤:**")
            for word, count in word_freq:
                st.markdown(f"- **{word.capitalize()}** ‚Äî {count} —Ä–∞–∑")

            plt.figure(figsize=(10, 4))
            plt.imshow(wc, interpolation="bilinear")
            plt.axis("off")
            st.pyplot(plt, clear_figure=True)
        else:
            st.info(f"üö´ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {tone}")

